{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8affc3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import requests as req\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "import time\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e97931c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eab168f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full URL: https://www.ncei.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&locationid=FIPS:19&startdate=2024-06-01&enddate=2024-06-30&datatypeid=TMAX,TMIN,PRCP&units=standard&limit=100\n"
     ]
    }
   ],
   "source": [
    "# Get NOAA token from environment variables\n",
    "NOAA_TOKEN = os.getenv(\"NOAA_TOKEN\")\n",
    "\n",
    "# Define parameters\n",
    "dataset = 'GHCND'\n",
    "location = 'FIPS:19'\n",
    "start = '2024-06-01'\n",
    "end = '2024-06-30'\n",
    "datatypes = 'TMAX,TMIN,PRCP'\n",
    "units = 'standard'\n",
    "limit = 100\n",
    "\n",
    "# Build URL with f-string\n",
    "url = f\"https://www.ncei.noaa.gov/cdo-web/api/v2/data?datasetid={dataset}&locationid={location}&startdate={start}&enddate={end}&datatypeid={datatypes}&units={units}&limit={limit}\"\n",
    "\n",
    "headers = {'token': NOAA_TOKEN}\n",
    "print(f\"Full URL: {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4075b69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = req.get(url, headers=headers, timeout=120)\n",
    "\n",
    "# Only parse JSON if status is 200\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "else:\n",
    "    print(\"\\nError : \", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "745e1cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (100, 5)\n",
      "┌─────────────────────┬──────────┬───────────────────┬────────────┬───────┐\n",
      "│ date                ┆ datatype ┆ station           ┆ attributes ┆ value │\n",
      "│ ---                 ┆ ---      ┆ ---               ┆ ---        ┆ ---   │\n",
      "│ str                 ┆ str      ┆ str               ┆ str        ┆ f64   │\n",
      "╞═════════════════════╪══════════╪═══════════════════╪════════════╪═══════╡\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAD0002 ┆ ,,N,0800   ┆ 0.0   │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAL0003 ┆ T,,N,0700  ┆ 0.0   │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAL0005 ┆ ,,N,0700   ┆ 0.0   │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAL0006 ┆ ,,N,0700   ┆ 0.01  │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAL0007 ┆ ,,N,0700   ┆ 0.0   │\n",
      "│ …                   ┆ …        ┆ …                 ┆ …          ┆ …     │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAEM0003 ┆ ,,N,0700   ┆ 1.0   │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAFM0001 ┆ ,,N,0800   ┆ 0.07  │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAFY0003 ┆ ,,N,0700   ┆ 0.0   │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAGN0004 ┆ ,,N,0600   ┆ 0.28  │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAGT0001 ┆ ,,N,0700   ┆ 0.14  │\n",
      "└─────────────────────┴──────────┴───────────────────┴────────────┴───────┘\n",
      "Unique datatypes: shape: (1,)\n",
      "Series: 'datatype' [str]\n",
      "[\n",
      "\t\"PRCP\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "df_raw = pl.DataFrame(data['results'])\n",
    "print(df_raw)\n",
    "print(\"Unique datatypes:\", df_raw['datatype'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc83db9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Iowa weather: 2024-06-01 to 2024-06-30\n",
      "Fetching offset 1... got 1000 records (total: 1000)\n",
      "Fetching offset 1001... got 1000 records (total: 2000)\n",
      "Fetching offset 2001... got 1000 records (total: 3000)\n",
      "Fetching offset 3001... got 1000 records (total: 4000)\n",
      "Fetching offset 4001... got 1000 records (total: 5000)\n",
      "Fetching offset 5001... got 1000 records (total: 6000)\n",
      "Fetching offset 6001... got 1000 records (total: 7000)\n",
      "Fetching offset 7001... got 1000 records (total: 8000)\n",
      "Fetching offset 8001... got 1000 records (total: 9000)\n",
      "Fetching offset 9001... got 1000 records (total: 10000)\n",
      "Fetching offset 10001... got 1000 records (total: 11000)\n",
      "Fetching offset 11001... got 1000 records (total: 12000)\n",
      "Fetching offset 12001... got 1000 records (total: 13000)\n",
      "Fetching offset 13001... \n",
      "Error :  503\n",
      "\n",
      "Total records fetched: 13000\n",
      "\n",
      "=== Final Dataset ===\n",
      "shape: (13_000, 5)\n",
      "┌─────────────────────┬──────────┬───────────────────┬────────────┬───────┐\n",
      "│ date                ┆ datatype ┆ station           ┆ attributes ┆ value │\n",
      "│ ---                 ┆ ---      ┆ ---               ┆ ---        ┆ ---   │\n",
      "│ str                 ┆ str      ┆ str               ┆ str        ┆ f64   │\n",
      "╞═════════════════════╪══════════╪═══════════════════╪════════════╪═══════╡\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAD0002 ┆ ,,N,0800   ┆ 0.0   │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAL0003 ┆ T,,N,0700  ┆ 0.0   │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAL0005 ┆ ,,N,0700   ┆ 0.0   │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAL0006 ┆ ,,N,0700   ┆ 0.01  │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAL0007 ┆ ,,N,0700   ┆ 0.0   │\n",
      "│ …                   ┆ …        ┆ …                 ┆ …          ┆ …     │\n",
      "│ 2024-06-20T00:00:00 ┆ TMAX     ┆ GHCND:USC00133007 ┆ ,,7,2400   ┆ 89.0  │\n",
      "│ 2024-06-20T00:00:00 ┆ TMIN     ┆ GHCND:USC00133007 ┆ ,,7,2400   ┆ 71.0  │\n",
      "│ 2024-06-20T00:00:00 ┆ PRCP     ┆ GHCND:USC00133120 ┆ ,,7,0600   ┆ 0.16  │\n",
      "│ 2024-06-20T00:00:00 ┆ PRCP     ┆ GHCND:USC00133438 ┆ ,,7,0800   ┆ 0.09  │\n",
      "│ 2024-06-20T00:00:00 ┆ TMAX     ┆ GHCND:USC00133438 ┆ ,,7,0800   ┆ 68.0  │\n",
      "└─────────────────────┴──────────┴───────────────────┴────────────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "def fetch_all_iowa_weather(start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch all Iowa weather data with pagination.\n",
    "    \"\"\"\n",
    "    NOAA_TOKEN = os.getenv('NOAA_TOKEN')\n",
    "\n",
    "    all_results = []\n",
    "    offset = 1\n",
    "    limit = 1000  # Max allowed by NOAA\n",
    "\n",
    "    base_url = \"https://www.ncei.noaa.gov/cdo-web/api/v2/data\"\n",
    "\n",
    "    print(f\"Fetching Iowa weather: {start_date} to {end_date}\")\n",
    "\n",
    "    while True:\n",
    "        # Build URL with current offset\n",
    "        url = f\"{base_url}?datasetid=GHCND&locationid=FIPS:19&startdate={start_date}&enddate={end_date}&datatypeid=TMAX,TMIN,PRCP&units=standard&limit={limit}&offset={offset}\"\n",
    "\n",
    "        headers = {'token': NOAA_TOKEN}\n",
    "\n",
    "        print(f\"Fetching offset {offset}...\", end=' ')\n",
    "\n",
    "        try:\n",
    "            response = req.get(url, headers=headers, timeout=300)\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print(\"\\nError : \", response.status_code)\n",
    "                break\n",
    "\n",
    "            data = response.json()\n",
    "\n",
    "            if 'results' not in data or len(data['results']) == 0:\n",
    "                print(\"No more results\")\n",
    "                break\n",
    "\n",
    "            batch_size = len(data['results'])\n",
    "            all_results.extend(data['results'])\n",
    "\n",
    "            print(f\"got {batch_size} records (total: {len(all_results)})\")\n",
    "\n",
    "            # Check if we've reached the end\n",
    "            if batch_size < limit:\n",
    "                print(\"Reached end of data\")\n",
    "                break\n",
    "\n",
    "            # Check metadata to see if there are more results\n",
    "            if 'metadata' in data and 'resultset' in data['metadata']:\n",
    "                total_count = data['metadata']['resultset']['count']\n",
    "                if len(all_results) >= total_count:\n",
    "                    print(f\"Fetched all {total_count} records\")\n",
    "                    break\n",
    "\n",
    "            offset += limit\n",
    "\n",
    "            # NOAA rate limit: 5 requests per second\n",
    "            # Be conservative: 4 requests per second\n",
    "            time.sleep(0.3)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {e}\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\nTotal records fetched: {len(all_results)}\")\n",
    "\n",
    "    # Convert to Polars DataFrame\n",
    "    df = pl.DataFrame(all_results)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Fetch all data for June 2024\n",
    "df_raw = fetch_all_iowa_weather('2024-06-01', '2024-06-30')\n",
    "\n",
    "print(\"\\n=== Final Dataset ===\")\n",
    "print(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e6d46b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_robust(\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    max_retries: int = 3,\n",
    "    checkpoint_file: str = 'weather_checkpoint.json'\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Weather fetcher more robust checkpointing in case of common 503 error\n",
    "    - retries transient errors (503, etc)\n",
    "    - checkpoints progress every N records to a file\n",
    "    - resumes from checkpoint if interrupted\n",
    "\n",
    "    Args:\n",
    "        start_date (str): Start date in YYYY-MM-DD format.\n",
    "        end_date (str): End date in YYYY-MM-DD format.\n",
    "        max_retries (int): Maximum number of retries for failed requests.\n",
    "        checkpoint_file (str): File to save checkpoint data.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame containing the fetched weather data.\n",
    "    \"\"\"\n",
    "    NOAA_TOKEN = os.getenv('NOAA_TOKEN')\n",
    "    checkpoint_path = Path(checkpoint_file)\n",
    "\n",
    "    # Load checkpoint if it exists\n",
    "    if checkpoint_path.exists():\n",
    "        with open(checkpoint_path, 'r') as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "        all_results = checkpoint_data['results']\n",
    "        offset = checkpoint_data['offset'] + 1000\n",
    "        print(f\"Resuming from checkpoint: {len(all_results)} records, offset {offset}\")\n",
    "    else:\n",
    "        print(\"Starting fresh fetch\")\n",
    "        all_results = []\n",
    "        offset = 1\n",
    "\n",
    "    base_url = 'https://www.ncei.noaa.gov/cdo-web/api/v2/data'\n",
    "    limit = 1000  # Max allowed by NOAA\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            url = f\"{base_url}?datasetid=GHCND&locationid=FIPS:19&startdate={start_date}&enddate={end_date}&datatypeid=TMAX,TMIN,PRCP&units=standard&limit={limit}&offset={offset}\"\n",
    "\n",
    "            # Retry logic\n",
    "            success = False\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    response = req.get(url, headers={'token': NOAA_TOKEN}, timeout=300)\n",
    "\n",
    "                    if response.status_code == 503:\n",
    "                        wait = 2 ** attempt\n",
    "                        print(f\"503 error, retrying in {wait} seconds...\")\n",
    "                        time.sleep(wait)\n",
    "                        continue\n",
    "\n",
    "                    if response.status_code != 200:\n",
    "                        print(\"\\nError : \", response.status_code)\n",
    "                        break\n",
    "\n",
    "                    data = response.json()\n",
    "\n",
    "                    if 'results' not in data or len(data['results']) == 0:\n",
    "                        print(\"Done\")\n",
    "                        success = True\n",
    "                        break\n",
    "\n",
    "                    batch_size = len(data['results'])\n",
    "                    all_results.extend(data['results'])\n",
    "\n",
    "                    print(f\"{batch_size} records (total: {len(all_results)})\")\n",
    "\n",
    "                    # Checkpoint every 5000 records\n",
    "                    if len(all_results) % 5000 < 1000:\n",
    "                        with open(checkpoint_path, 'w') as f:\n",
    "                            json.dump({\n",
    "                                'start_date': start_date,\n",
    "                                'end_date': end_date,\n",
    "                                'last_offset': offset,\n",
    "                                'total_records': len(all_results),\n",
    "                                'results': all_results\n",
    "                            }, f)\n",
    "                        print(\"Saved\")\n",
    "\n",
    "                    if batch_size < limit:\n",
    "                        success = True\n",
    "                        break\n",
    "\n",
    "                    offset += limit\n",
    "                    time.sleep(0.3)  # Rate limiting\n",
    "                    success = True\n",
    "                    break\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}, attempt {attempt + 1} of {max_retries}\")\n",
    "                    if attempt < max_retries - 1:\n",
    "                        time.sleep(2 ** attempt)\n",
    "\n",
    "            if not success or (data and ('results' not in data or len(data['results']) == 0)):\n",
    "                break\n",
    "\n",
    "        # Clean up on success\n",
    "        if checkpoint_path.exists():\n",
    "            checkpoint_path.unlink()\n",
    "\n",
    "        return pl.DataFrame(all_results)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted, saving checkpoint...\")\n",
    "        with open(checkpoint_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'start_date': start_date,\n",
    "                'end_date': end_date,\n",
    "                'last_offset': offset - 1000,\n",
    "                'total_records': len(all_results),\n",
    "                'results': all_results\n",
    "            }, f)\n",
    "        print(\"Checkpoint saved. Exiting.\")\n",
    "        return pl.DataFrame(all_results) if all_results else pl.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5328c153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fresh fetch\n",
      "1000 records (total: 1000)\n",
      "1000 records (total: 2000)\n",
      "1000 records (total: 3000)\n",
      "1000 records (total: 4000)\n",
      "1000 records (total: 5000)\n",
      "Saved\n",
      "1000 records (total: 6000)\n",
      "1000 records (total: 7000)\n",
      "1000 records (total: 8000)\n",
      "1000 records (total: 9000)\n",
      "1000 records (total: 10000)\n",
      "Saved\n",
      "1000 records (total: 11000)\n",
      "1000 records (total: 12000)\n",
      "1000 records (total: 13000)\n",
      "503 error, retrying in 1 seconds...\n",
      "1000 records (total: 14000)\n",
      "1000 records (total: 15000)\n",
      "Saved\n",
      "1000 records (total: 16000)\n",
      "1000 records (total: 17000)\n",
      "1000 records (total: 18000)\n",
      "1000 records (total: 19000)\n",
      "900 records (total: 19900)\n",
      "900 records (total: 20800)\n",
      "Saved\n",
      "900 records (total: 21700)\n",
      "900 records (total: 22600)\n",
      "900 records (total: 23500)\n",
      "503 error, retrying in 1 seconds...\n",
      "900 records (total: 24400)\n",
      "900 records (total: 25300)\n",
      "Saved\n",
      "900 records (total: 26200)\n",
      "900 records (total: 27100)\n",
      "900 records (total: 28000)\n",
      "900 records (total: 28900)\n",
      "900 records (total: 29800)\n",
      "900 records (total: 30700)\n",
      "Saved\n",
      "900 records (total: 31600)\n",
      "900 records (total: 32500)\n",
      "900 records (total: 33400)\n",
      "900 records (total: 34300)\n",
      "503 error, retrying in 1 seconds...\n",
      "900 records (total: 35200)\n",
      "Saved\n",
      "900 records (total: 36100)\n",
      "503 error, retrying in 1 seconds...\n",
      "900 records (total: 37000)\n",
      "900 records (total: 37900)\n",
      "900 records (total: 38800)\n",
      "900 records (total: 39700)\n",
      "900 records (total: 40600)\n",
      "Saved\n",
      "900 records (total: 41500)\n",
      "900 records (total: 42400)\n",
      "900 records (total: 43300)\n",
      "900 records (total: 44200)\n",
      "503 error, retrying in 1 seconds...\n",
      "900 records (total: 45100)\n",
      "Saved\n",
      "900 records (total: 46000)\n",
      "503 error, retrying in 1 seconds...\n",
      "900 records (total: 46900)\n",
      "\n",
      "Error :  502\n",
      "\n",
      "=== Robust Fetch Dataset ===\n",
      "shape: (46_900, 5)\n",
      "┌─────────────────────┬──────────┬───────────────────┬────────────┬───────┐\n",
      "│ date                ┆ datatype ┆ station           ┆ attributes ┆ value │\n",
      "│ ---                 ┆ ---      ┆ ---               ┆ ---        ┆ ---   │\n",
      "│ str                 ┆ str      ┆ str               ┆ str        ┆ f64   │\n",
      "╞═════════════════════╪══════════╪═══════════════════╪════════════╪═══════╡\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAD0002 ┆ ,,N,0800   ┆ 0.0   │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAL0003 ┆ T,,N,0700  ┆ 0.0   │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAL0005 ┆ ,,N,0700   ┆ 0.0   │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAL0006 ┆ ,,N,0700   ┆ 0.01  │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAL0007 ┆ ,,N,0700   ┆ 0.0   │\n",
      "│ …                   ┆ …        ┆ …                 ┆ …          ┆ …     │\n",
      "│ 2024-06-30T00:00:00 ┆ TMAX     ┆ GHCND:USW00094989 ┆ ,,W,2400   ┆ 77.0  │\n",
      "│ 2024-06-30T00:00:00 ┆ TMIN     ┆ GHCND:USW00094989 ┆ ,,W,2400   ┆ 55.0  │\n",
      "│ 2024-06-30T00:00:00 ┆ PRCP     ┆ GHCND:USW00094991 ┆ ,,W,2400   ┆ 0.0   │\n",
      "│ 2024-06-30T00:00:00 ┆ TMAX     ┆ GHCND:USW00094991 ┆ ,,W,2400   ┆ 76.0  │\n",
      "│ 2024-06-30T00:00:00 ┆ TMIN     ┆ GHCND:USW00094991 ┆ ,,W,2400   ┆ 55.0  │\n",
      "└─────────────────────┴──────────┴───────────────────┴────────────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "df = fetch_robust('2024-06-01', '2024-06-30')\n",
    "print(\"\\n=== Robust Fetch Dataset ===\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2c012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import polars as pl\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, wait, FIRST_COMPLETED\n",
    "import time\n",
    "from threading import Lock\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def fetch_batch_safe(offset, start_date, end_date, token, max_retries=3):\n",
    "    \"\"\"Fetch a single batch with retries.\"\"\"\n",
    "    url = f\"https://www.ncei.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&locationid=FIPS:19&startdate={start_date}&enddate={end_date}&datatypeid=TMAX,TMIN,PRCP&units=standard&limit=1000&offset={offset}\"\n",
    "\n",
    "    headers = {'token': token}\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=120)\n",
    "\n",
    "            if response.status_code == 503:\n",
    "                wait_time = 2 ** attempt\n",
    "                print(f\"  503 at offset {offset}, retry in {wait_time}s\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "\n",
    "            if response.status_code == 502:\n",
    "                return (offset, None, 0)\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                return (offset, None, 0)\n",
    "\n",
    "            data = response.json()\n",
    "\n",
    "            if 'results' not in data:\n",
    "                return (offset, [], 0)\n",
    "\n",
    "            results = data['results']\n",
    "            return (offset, results, len(results))\n",
    "\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "            else:\n",
    "                return (offset, None, 0)\n",
    "\n",
    "    return (offset, None, 0)\n",
    "\n",
    "\n",
    "def fetch_concurrent_sliding_window(start_date, end_date, max_workers=4):\n",
    "    \"\"\"\n",
    "    Fetch with proper sliding window - FIXED VERSION.\n",
    "\n",
    "    Key fix: Use while loop + wait() instead of as_completed()\n",
    "    \"\"\"\n",
    "    NOAA_TOKEN = os.getenv('NOAA_TOKEN')\n",
    "\n",
    "    if not NOAA_TOKEN:\n",
    "        print(\"ERROR: NOAA_TOKEN not found!\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Fetching Iowa weather: {start_date} to {end_date} (max {max_workers} workers)\")\n",
    "\n",
    "    all_results = []\n",
    "    next_offset = 1\n",
    "    empty_batches = 0\n",
    "    max_empty = 3\n",
    "\n",
    "    results_lock = Lock()\n",
    "    start_time = time.monotonic()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        active_futures = {}\n",
    "\n",
    "        # Submit initial batch\n",
    "        for i in range(max_workers):\n",
    "            offset = next_offset\n",
    "            future = executor.submit(fetch_batch_safe, offset, start_date, end_date, NOAA_TOKEN)\n",
    "            active_futures[future] = offset\n",
    "            next_offset += 1000\n",
    "\n",
    "        print(f\"Submitted initial {len(active_futures)} requests\\n\")\n",
    "\n",
    "        # Process completions with while loop\n",
    "        while active_futures:\n",
    "            # Wait for at least one to complete\n",
    "            done, pending = wait(active_futures.keys(), return_when=FIRST_COMPLETED, timeout=120)\n",
    "\n",
    "            if not done:\n",
    "                print(\"Timeout waiting for futures\")\n",
    "                break\n",
    "\n",
    "            # Process all completed futures\n",
    "            for future in done:\n",
    "                offset = active_futures.pop(future)\n",
    "\n",
    "                try:\n",
    "                    fetch_offset, results, batch_size = future.result()\n",
    "\n",
    "                    if results is None:\n",
    "                        empty_batches += 1\n",
    "                        print(f\"Offset {fetch_offset:,}: Failed\")\n",
    "\n",
    "                        if empty_batches >= max_empty:\n",
    "                            print(f\"Stopping after {max_empty} failures\")\n",
    "                            # Cancel remaining\n",
    "                            for f in active_futures.keys():\n",
    "                                f.cancel()\n",
    "                            active_futures.clear()\n",
    "                            break\n",
    "\n",
    "                    elif batch_size == 0:\n",
    "                        empty_batches += 1\n",
    "                        print(f\"Offset {fetch_offset:,}: Empty\")\n",
    "\n",
    "                        if empty_batches >= max_empty:\n",
    "                            print(f\"End of data (after {max_empty} empty batches)\")\n",
    "                            for f in active_futures.keys():\n",
    "                                f.cancel()\n",
    "                            active_futures.clear()\n",
    "                            break\n",
    "\n",
    "                    else:\n",
    "                        # Success!\n",
    "                        empty_batches = 0\n",
    "\n",
    "                        with results_lock:\n",
    "                            all_results.extend(results)\n",
    "\n",
    "                        print(f\"Offset {fetch_offset:,}: {batch_size} records (total: {len(all_results):,})\")\n",
    "\n",
    "                        # Submit next batch (keep window full)\n",
    "                        if next_offset < 100000:  # Safety limit\n",
    "                            new_future = executor.submit(fetch_batch_safe, next_offset, start_date, end_date, NOAA_TOKEN)\n",
    "                            active_futures[new_future] = next_offset\n",
    "                            next_offset += 1000\n",
    "\n",
    "                        time.sleep(0.25)  # Rate limit\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Offset {offset:,}: Exception {e}\")\n",
    "                    empty_batches += 1\n",
    "\n",
    "            # Check stopping condition\n",
    "            if empty_batches >= max_empty:\n",
    "                break\n",
    "\n",
    "    elapsed = time.monotonic() - start_time\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Completed in {elapsed:.1f}s ({elapsed/60:.1f} min)\")\n",
    "    print(f\"Total records: {len(all_results):,}\")\n",
    "    print(f\"Speed: {len(all_results) / elapsed:.0f} records/sec\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    return pl.DataFrame(all_results) if all_results else None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b060770e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING CONCURRENT FETCH\n",
      "============================================================\n",
      "Fetching Iowa weather: 2024-06-01 to 2024-06-30 (max 4 workers)\n",
      "Submitted initial 4 requests\n",
      "\n",
      "  503 at offset 3001, retry in 1s\n",
      "Offset 1: 1000 records (total: 1,000)\n",
      "Offset 2,001: 1000 records (total: 2,000)\n",
      "Offset 1,001: 1000 records (total: 3,000)\n",
      "  503 at offset 4001, retry in 1s\n",
      "Offset 3,001: 1000 records (total: 4,000)\n",
      "Offset 5,001: 1000 records (total: 5,000)\n",
      "Offset 6,001: 1000 records (total: 6,000)\n",
      "Offset 4,001: 1000 records (total: 7,000)\n",
      "Offset 7,001: 1000 records (total: 8,000)\n",
      "Offset 9,001: 1000 records (total: 9,000)\n",
      "Offset 8,001: 1000 records (total: 10,000)\n",
      "Offset 10,001: 1000 records (total: 11,000)\n",
      "Offset 11,001: 1000 records (total: 12,000)\n",
      "Offset 12,001: 1000 records (total: 13,000)\n",
      "Offset 13,001: 1000 records (total: 14,000)\n",
      "Offset 15,001: 1000 records (total: 15,000)\n",
      "Offset 14,001: 1000 records (total: 16,000)\n",
      "Offset 17,001: 1000 records (total: 17,000)\n",
      "Offset 16,001: 1000 records (total: 18,000)\n",
      "Offset 19,001: 900 records (total: 18,900)\n",
      "Offset 18,001: 1000 records (total: 19,900)\n",
      "  503 at offset 21001, retry in 1s\n",
      "Offset 20,001: Empty\n",
      "Offset 22,001: Empty\n",
      "Offset 23,001: Empty\n",
      "End of data (after 3 empty batches)\n",
      "\n",
      "============================================================\n",
      "Completed in 477.7s (8.0 min)\n",
      "Total records: 19,900\n",
      "Speed: 42 records/sec\n",
      "============================================================\n",
      "\n",
      "=== Final Dataset ===\n",
      "Shape: (19900, 5)\n",
      "shape: (10, 5)\n",
      "┌─────────────────────┬──────────┬───────────────────┬────────────┬───────┐\n",
      "│ date                ┆ datatype ┆ station           ┆ attributes ┆ value │\n",
      "│ ---                 ┆ ---      ┆ ---               ┆ ---        ┆ ---   │\n",
      "│ str                 ┆ str      ┆ str               ┆ str        ┆ f64   │\n",
      "╞═════════════════════╪══════════╪═══════════════════╪════════════╪═══════╡\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAD0002 ┆ ,,N,0800   ┆ 0.0   │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAL0003 ┆ T,,N,0700  ┆ 0.0   │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAL0005 ┆ ,,N,0700   ┆ 0.0   │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAL0006 ┆ ,,N,0700   ┆ 0.01  │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAL0007 ┆ ,,N,0700   ┆ 0.0   │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAP0009 ┆ ,,N,0712   ┆ 0.23  │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAP0011 ┆ ,,N,0800   ┆ 0.0   │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAP0012 ┆ ,,N,0700   ┆ 0.18  │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IABC0009 ┆ ,,N,0900   ┆ 0.12  │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IABC0011 ┆ ,,N,0700   ┆ 0.0   │\n",
      "└─────────────────────┴──────────┴───────────────────┴────────────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "# Run with debug output\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING CONCURRENT FETCH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df = fetch_concurrent_sliding_window('2024-06-01', '2024-06-30', max_workers=4)\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"\\n=== Final Dataset ===\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(df.head(10))\n",
    "else:\n",
    "    print(\"\\nNo data returned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7576288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Concurrent Sliding Window Fetch Dataset ===\n",
      "shape: (19_900, 5)\n",
      "┌─────────────────────┬──────────┬───────────────────┬────────────┬───────┐\n",
      "│ date                ┆ datatype ┆ station           ┆ attributes ┆ value │\n",
      "│ ---                 ┆ ---      ┆ ---               ┆ ---        ┆ ---   │\n",
      "│ str                 ┆ str      ┆ str               ┆ str        ┆ f64   │\n",
      "╞═════════════════════╪══════════╪═══════════════════╪════════════╪═══════╡\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAD0002 ┆ ,,N,0800   ┆ 0.0   │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAL0003 ┆ T,,N,0700  ┆ 0.0   │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAL0005 ┆ ,,N,0700   ┆ 0.0   │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAL0006 ┆ ,,N,0700   ┆ 0.01  │\n",
      "│ 2024-06-01T00:00:00 ┆ PRCP     ┆ GHCND:US1IAAL0007 ┆ ,,N,0700   ┆ 0.0   │\n",
      "│ …                   ┆ …        ┆ …                 ┆ …          ┆ …     │\n",
      "│ 2024-06-29T00:00:00 ┆ TMIN     ┆ GHCND:USC00133487 ┆ ,,7,0700   ┆ 61.0  │\n",
      "│ 2024-06-29T00:00:00 ┆ TMAX     ┆ GHCND:USC00133509 ┆ ,,7,0800   ┆ 84.0  │\n",
      "│ 2024-06-29T00:00:00 ┆ TMIN     ┆ GHCND:USC00133509 ┆ ,,7,0800   ┆ 68.0  │\n",
      "│ 2024-06-29T00:00:00 ┆ PRCP     ┆ GHCND:USC00133517 ┆ ,,7,0830   ┆ 0.68  │\n",
      "│ 2024-06-29T00:00:00 ┆ TMAX     ┆ GHCND:USC00133517 ┆ ,,7,0830   ┆ 72.0  │\n",
      "└─────────────────────┴──────────┴───────────────────┴────────────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Concurrent Sliding Window Fetch Dataset ===\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc547ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual date range in data:\n",
      "Earliest: 2024-06-01\n",
      "Latest: 2024-06-30\n",
      "\n",
      "Records per day:\n",
      "shape: (30, 2)\n",
      "┌─────────────┬─────────────┐\n",
      "│ date_parsed ┆ num_records │\n",
      "│ ---         ┆ ---         │\n",
      "│ date        ┆ u32         │\n",
      "╞═════════════╪═════════════╡\n",
      "│ 2024-06-01  ┆ 674         │\n",
      "│ 2024-06-02  ┆ 669         │\n",
      "│ 2024-06-03  ┆ 684         │\n",
      "│ 2024-06-04  ┆ 659         │\n",
      "│ 2024-06-05  ┆ 717         │\n",
      "│ …           ┆ …           │\n",
      "│ 2024-06-26  ┆ 681         │\n",
      "│ 2024-06-27  ┆ 647         │\n",
      "│ 2024-06-28  ┆ 689         │\n",
      "│ 2024-06-29  ┆ 670         │\n",
      "│ 2024-06-30  ┆ 638         │\n",
      "└─────────────┴─────────────┘\n",
      "\n",
      "June 30 records: 1\n"
     ]
    }
   ],
   "source": [
    "# Check what dates are actually in your data\n",
    "date_min = df['date'].str.strptime(pl.Date, '%Y-%m-%dT%H:%M:%S').min()\n",
    "date_max = df['date'].str.strptime(pl.Date, '%Y-%m-%dT%H:%M:%S').max()\n",
    "\n",
    "print(\"Actual date range in data:\")\n",
    "print(f\"Earliest: {date_min}\")\n",
    "print(f\"Latest: {date_max}\")\n",
    "\n",
    "# Count records per day\n",
    "daily_counts = (\n",
    "    df\n",
    "    .with_columns([\n",
    "        pl.col('date').str.strptime(pl.Date, '%Y-%m-%dT%H:%M:%S').alias('date_parsed')\n",
    "    ])\n",
    "    .group_by('date_parsed')\n",
    "    .agg([\n",
    "        pl.len().alias('num_records')\n",
    "    ])\n",
    "    .sort('date_parsed')\n",
    ")\n",
    "\n",
    "print(\"\\nRecords per day:\")\n",
    "print(daily_counts)\n",
    "\n",
    "# Check if June 30 exists\n",
    "june_30_count = daily_counts.filter(pl.col('date_parsed') == pl.date(2024, 6, 30))\n",
    "print(f\"\\nJune 30 records: {len(june_30_count)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19583ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write_parquet('C:\\\\Users\\\\samb2\\\\Documents\\\\GitHub\\\\ag-data-engineering-dashboard\\\\data\\\\raw\\\\iowa_weather_june2024.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09eeb85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
